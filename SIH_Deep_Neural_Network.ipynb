{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIH_Deep_Neural_Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPGyZJ0Xs7WC2Wn37tEPnW4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hari-19/Smart_India_Hackathon-2020/blob/master/SIH_Deep_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApTEzLDLLYrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ssUL1pebAV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Set needs to be Loaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W-lhE9FNJ_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Returns tanh of Data\n",
        "\n",
        "def tanh(data):\n",
        "  data = np.tanh(data)\n",
        "  return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vVmAGTKOXN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Returns ReLU of Data\n",
        "\n",
        "def relu(data):\n",
        "    data = np.where(data>0,data,0)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQHaipuzfpK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Returns Sigmoid of Data\n",
        "\n",
        "def sigmoid(data):\n",
        "    data = 1 + np.exp(-data)\n",
        "    data = 1/data\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MUavT2SQ7zZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parameter initialization\n",
        "\n",
        "    # Arguments:\n",
        "    # layer_dims -- python array (list) containing the dimensions of each layer in the network\n",
        "    \n",
        "    # Returns:\n",
        "    # parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "    #                 Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "    #                 bl -- bias vector of shape (layer_dims[l], 1)\n",
        "\n",
        "def initialize_parameters(layer_dims):\n",
        "    parameters={}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1)) \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQjmONqKeBXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # The linear part of a layer's forward propagation.\n",
        "\n",
        "    # Arguments:\n",
        "    # A -- activations from previous layer : (size of previous layer, number of examples)\n",
        "    # W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    # b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    # Returns:\n",
        "    # Z -- the input of the activation function\n",
        "    # cache -- a python tuple containing \"A\", \"W\" and \"b\"\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    Z = np.dot(W,A) + b\n",
        "\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mssn2XNtefec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # The forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    # Arguments:\n",
        "    # A_prev -- activations from previous layer : (size of previous layer, number of examples)\n",
        "    # W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    # b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    # activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\" or \"tanh\"(will be coded later)\n",
        "\n",
        "    # Returns:\n",
        "    # A -- the output of the activation function\n",
        "    # cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "   \n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, Z)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPz2XqBhf-ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    # Forward propagation for the [Linear --> Sigmoid] * (L-1) -> [LINEAR->ReLU] computation\n",
        "    \n",
        "    # Arguments:\n",
        "    # X -- data, numpy array of shape (input size, number of examples)\n",
        "    # parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    # Returns:\n",
        "    # AL -- last post-activation value\n",
        "    # caches -- list of caches containing:\n",
        "    #             every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"sigmoid\")\n",
        "        caches.append(cache)\n",
        "\n",
        "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"relu\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0cKeHjAPH-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    #Cost Function\n",
        "    #Squared Error\n",
        "\n",
        "    # Arguments:\n",
        "    # AL -- probability vector corresponding to predictions, shape (1, number of examples)\n",
        "    # Y -- true value\n",
        "\n",
        "    # Returns:\n",
        "    # cost -- cross-entropy cost\n",
        "\n",
        "def compute_cost(AL, Y):  \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    cost = (-1/(2 * m)) * np.sum(np.square(Y - AL) )\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spWMPF7lPzCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # The Linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    # Arguments:\n",
        "    # dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    # cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    # Returns:\n",
        "    # dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    # dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    # db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1/m) * np.dot(dZ, np.transpose(A_prev))\n",
        "    db = (1/m) * np.sum(dZ,axis = 1,keepdims = True)\n",
        "    dA_prev = np.dot(np.transpose(W), dZ)\n",
        "   \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxF0SH4cTJ8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # The backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    # Arguments:\n",
        "    # dA -- post-activation gradient for current layer l \n",
        "    # cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    # activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    # Returns:\n",
        "    # dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    # dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    # db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "   \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTC-9c3HTceE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The backward propagation for the [LINEAR->Sigmoid] * (L-1) -> [LINEAR -> ReLU] group\n",
        "    \n",
        "#     Arguments:\n",
        "#     AL -- Output of the forward propagation (L_model_forward())\n",
        "#     Y -- True Value\n",
        "#     caches -- list of caches containing:\n",
        "\n",
        "    \n",
        "#     Returns:\n",
        "#     grads -- A dictionary with the gradients\n",
        "#              grads[\"dA\" + str(l)] = ... \n",
        "#              grads[\"dW\" + str(l)] = ...\n",
        "#              grads[\"db\" + str(l)] = ... \n",
        "\n",
        "def L_model_backward(AL, Y, caches):\n",
        "\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = Y - AL # derivative of cost with respect to AL\n",
        "    \n",
        "    # Lth layer (ReLU -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"relu\")\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (Sigmoid -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, \"sigmoid\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-TK-cumYfUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # Update parameters using gradient descent\n",
        "    \n",
        "    # Arguments:\n",
        "    # parameters -- python dictionary containing your parameters \n",
        "    # grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    # Returns:\n",
        "    # parameters -- python dictionary containing your updated parameters \n",
        "    #               parameters[\"W\" + str(l)] = ... \n",
        "    #               parameters[\"b\" + str(l)] = ...\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "\n",
        "    L = len(parameters) // # number of layers in the neural network\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - np.multiply(learning_rate, grads[\"dW\" + str(l+1)])\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - np.multiply(learning_rate, grads[\"db\" + str(l+1)])\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}